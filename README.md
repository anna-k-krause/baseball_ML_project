# PythonProject

# Setup for developement:

- Setup a python 3.x venv (usually in `.venv`)
  - You can run `./scripts/create-venv.sh` to generate one
- `pip3 install --upgrade pip`
- Install pip-tools `pip3 install pip-tools`
- Update dev requirements: `pip-compile --output-file=requirements.dev.txt requirements.dev.in`
- Update requirements: `pip-compile --output-file=requirements.txt requirements.in`
- Install dev requirements `pip3 install -r requirements.dev.txt`
- Install requirements `pip3 install -r requirements.txt`
- `pre-commit install`

## Update versions

`pip-compile --output-file=requirements.dev.txt requirements.dev.in --upgrade`
`pip-compile --output-file=requirements.txt requirements.in --upgrade`

# Run `pre-commit` locally.

`pre-commit run --all-files`

## Baseball Final Project - Anna Krause


# Intro + Requirements

I decided to take a baseball dataset [Link](https://teaching.mrsharky.com/sdsu_fall_2020_lecture04.html#/10/1) and predict if the home team would win over the away team. To conduct this analysis I used SQL, Python, and Docker to generate features, conduct feature engineering, and build and train machine learning models. I also generated multiple visualizations to help with the feature engineering process. This wiki will walk you through my entire process, with sources linked throughout. 

# Generating Features 

I generated 30 predictor features and 1 response feature from baseball.sql. I generated the response (HomeTeamWins) by taking the ‘win’ column from the team_batting_counts table. I checked that this win vs loss record was correct by also checking the finalScore vs opponent_finalScore columns against each other, and found that the win column was accurate to use as the response. I also only took the games from this table where the HomeTeam = 1, as there are repeating rows for the away team as well for each unique gameid. I utilized mariadb as my version of SQL for this work. 

I generated nearly all of my batting stats from this [Link](https://en.wikipedia.org/wiki/Baseball_statistics). All of these values were taken from the team_batting_counts table. For each feature besides ‘rolling_high_scoring_game_diff’, I found a 100 day rolling average of the home team and the away team. I then subtracted the away team’s average from the home team’s average. This resulted in a rolling average differential leading up to each game. To calculate the rolling averages for each feature, I joined and summed up the components of each feature, and then performed all of the necessary calculations. The batting features I found were batting average (BA), walk to strikeout ratio (BB/K), ground balls to fly balls ratio (GOAO), plate appearances to strikeouts ratio (PA/SO), home runs to hits ratio (HR/R), on base percentage (OBP), at bats to home runs (AB/HR), times on base (TOB), slugging percentage (SLG), isolated power (ISO), and gross production average (GPA). My final batting stat was a rolling high scoring game difference. This was a feature I created based on how many total runs were scored in each game (combining both home and away runs). I found that over 10 runs scored in each game was considered “high scoring,” and wanted to see if it would be a compelling feature for my model.[Link](https://tht.fangraphs.com/runs-per-game/) If a game was high scoring I assigned it a 1 and if not, 0. I took a 100 day rolling sum of these high scoring game values in a similar fashion to the rolling averages of the other batting features. 

I generated nearly all of my pitching stats from the same source as the batting stats. All values were taken from the pitching_counts table and I only looked at the starting pitcher’s information for each game. I followed the same practice as the batting stats and found the difference between the home and away team’s 100 day rolling average leading up to each game (except for the rolling quality start feature I made). The pitching features I found were walks allowed per 9 innings pitched (BB/9), hits allowed per 9 innings pitched (H/9), home runs allowed per 9 innings pitched (HR/9), strikeouts allowed per 9 innings pitched (K/9), walks and hits per inning pitched (WHIP), power fitness ratio (PFR), Defense-independent component ERA (DICE), strikeout to walk ratio (K/BB), and ground outs to fly balls (GOAO). I also came up with 2 other rolling average pitching stats of my own. I compared intended walks to walks per inning pitched, and the strikeouts to pitches thrown. I was inspired by Barry Bonds' outrageous number of career intentional walks, and wanted to see if it made a difference in my models. [Link](https://en.wikipedia.org/wiki/List_of_Major_League_Baseball_career_intentional_bases_on_balls_leaders#:~:text=Barry%20Bonds%20is%20the%20all,career%20intentional%20bases%20on%20balls.) I also found the 100 day rolling sum of a starting teams’ pitchers’ quality starts (QS). 

I generated my environmental features by looking at the boxscore and game tables. These features all are related to the weather, environment, and setting of the game. I found the stadium id, and day/night status of the game from the games table. A game is considered a “night” game if it begins after 7pm local time. [Link](https://www.fantasylabs.com/articles/mlb-day-games-vs-night-games/#:~:text=For%20the%20purposes%20of%20this,00%20pm%20ET%20or%20later) I used the local date column and filtered on hour to make this distinction. I generated the indoor vs outdoor game distinction from the wind column in the boxscore table as game_environment. I found the game temperature feature from filtering the temp column. I grabbed the game wind direction and game weather columns as well from boxscore to add as features. 

I then joined all of these features together, adding indexes to my temporary tables when necessary to speed up the process. This resulted in one final table that would be exported to python. Throughout this cleaning process, I noticed that a few columns are spelled incorrectly. I also combined similar columns like Fly_Out and Flyout, as when I checked their status on certain games, they seemed to complement each other. So I summed these similar columns for my features. There was no distinction between earned runs and home runs, which made a few of my pitching statistics not “true” to how the baseball gods intended, but I made do with what I had. There was one game with 2 starting pitchers, and one game with a temperature of over 7000. Be on the lookout for similar little mistakes like these when generating your features. 

# Feature Engineering
I used python to conduct the rest of my analysis on predicting if the home team would win from the data available to me from the baseball dataset. I used sql alchemy to connect to my mariadb database and the sql code I generated. [Link](https://teaching.mrsharky.com/sdsu_fall_2020_lecture04.html#/7/3) I selected all of my features from my final table and put them into a pandas dataframe. I assigned my response as the home team winning column, and the rest of the features as the predictors. I encountered less than 10 rows with a null in their data, so I ended up just dropping those rows, since the columns with missing data varied and a unique modification for each row did not seem necessary when so little data was missing in the first place. This approach should not be used with larger swaths of missing data, but for my case it seemed appropriate. I then assigned each predictor and the response to a specific data type, as this will help with feature engineering steps later. I determined that my response was boolean, and I had a mix of 5 categorical predictors and 25 continuous predictors. 

I generated mean of response values and plots for my continuous and categorical predictors to determine if they would be valuable predictors for the model. For the continuous predictors, I generated a histogram and assigned 10 bins for each predictor. For the categorical predictors, I used the number of variables within the feature as the number of bins. I found the overall mean for each predictor, and then the mean of each generated bin. These were plotted along with the population of each bin. I used plotly to generate these visuals. I also generated the aggregated difference with mean of response, and the weighted aggregated difference with mean of response. The formulas are in this link. [Link](https://teaching.mrsharky.com/sdsu_fall_2020_lecture07.html#/6/0/6) 

I also generated plotly violin plots for my continuous predictors and density heatmaps for my categorical predictors against the response to test their usability in the models. I then found the p and t values for each continuous predictor using a logistic regression model from statsmodels. [Link](https://www.geeksforgeeks.org/logistic-regression-using-statsmodels/)
I then found the random forest feature importances of each continuous predictor, as these may also help us determine quality features to include in our model. This is a component of the sk-learn Random Forest model. [Link](https://mljar.com/blog/feature-importance-in-random-forest/)

## Correlations

I then split up my features to compare them in 3 different correlation categories, with continuous/continuous, categorical/categorical/ and continuous/categorical. To find the correlation of cont/cont I used the pearson’s R from scipy.stats. [Link](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html)
For cat/cat I used both the Cramer and Tschuprow correlation variables to test between predictors. [Link](https://teaching.mrsharky.com/sdsu_fall_2020_lecture08.html#/6/0/10) For cat/cont I used the correlation ratio outlined here. [Link](https://teaching.mrsharky.com/sdsu_fall_2020_lecture08.html#/6/0/10) For each of these comparisons, I also generated a plotly heatmap to show all of the predictors against each other in each of the 3 categories. 

I then found the brute force correlations between all three correlation types outlined above. Each had a mean of response plot generated for them again, but this time in the form of a heatmap with the x and y being the two compared features, and the z being the mean of response for each of the two features. Each continuous variable had 10 bins, and each categorical predictor had the same number of bins as the number of variables within itself. I used plotly to generate each heatmap. I also generated the aggregated difference with mean of response, and the weighted aggregated difference with mean of response for each pair of predictors. 

To display my feature findings, correlations, and brute force correlations, I combined all of my findings into a dataframe. The initial findings contain each feature, its data type, t_value, p_value, DMR, wDMR, the DMR plot, the violin or heatmap plot, and the generated random forest importance. For each correlation dataframe, each row has the two compared features, their correlation (pearson for cont/cont, cramer and tschuprow for cat/cat, or the found correlation for cat/cont), and their respective DMR plots. I included each correlation type’s 3 dimensional heatmap as well in this output report. These reports will help determine which features should be included in the final generated ML model. I ordered my features by wDMR (highest to lowest), as this variable helps determine which features are the most predictive. I also examined their p value, as a very low p value also means a feature is likely predictive. When looking at the correlations, I ordered my findings by highest to lowest correlation coefficients to determine which features to keep or discard. Features with high correlation (especially if one of them is less predictive) is a good way to decide to drop that feature and just keep the more predictive one. 

For the most part, the values with the highest wDMR also had very low p scores, so I used both of these to help pick which features to keep. I found that for the most part, pitching statistics had the highest wDMR and lowest p values. Features like PFR, strikeouts allowed, home runs allowed, hits allowed, and whip were the most predictive. Stadium ID was the most predictive categorical variable. This makes sense as certain stadiums are known as “hitter’s parks” and often result in more home runs compared to other parks. When comparing correlations between predictors, I found that many statistics within each category (batting, pitching, and environmental) were correlated with each other. For the batting and pitching stats, this is likely because many of the same components (hits, home runs, strikeouts, walks, innings pitched, etc) are present across calculated metrics and many of the features I created are just built off of other already created metrics. Unsurprisingly, weather metrics were highly correlated with other weather metrics, showing that most of them were unneeded in my model. 

# Models + Wrap up

For my modeling, I used sk-learn for the preprocessing and model construction. I separated my categorical and continuous features and ran them through a sk-learn pipeline. [Link](https://stackoverflow.com/questions/48673402/how-can-i-standardize-only-numeric-variables-in-an-sklearn-pipeline) I used standard scalar to transform continuous features and a one hot encoder to transform categorical features. I compared a few classifier models from sklearn to test my features' predictive power. [Link](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) I then split my data in a train test split of 80/20, and made sure the last 20% of the data was the last 20% of the games played. My data was already organized by gameid and localdate so they were in chronological order. I set my target as the response, HomeTeamWins, and the rest of my chosen features as the predictors. I then fit each of my chosen models (Logistic regression, random forest, gaussian naive bayes, svm, gradient boosted tree, and k-nearest neighbors) to the training data. I had these models then predict on the set aside test data, and see how their results compared to the real test target results. I used accuracy score, precision, and area under the roc curve to test my model performance. [Link](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)
My initial findings with a features showed that the roc_auc score was highest for the gradient boosted tree (.6797) and the random forest (.6730). The rest of the results were not as desirable. I tested my models with fewer features, but all of the results with trimmed features had worse model performance, so I ended up just keeping all of my features to test. 

I also set up docker and docker-compose for all of my work to attempt to retain the same environment for others to test their work and end up with the same results. Thanks for reading! -Anna 


